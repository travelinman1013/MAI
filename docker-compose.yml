services:
  redis:
    image: redis:7-alpine
    container_name: mai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped
    networks:
      - mai-network

  postgres:
    image: pgvector/pgvector:pg18
    container_name: mai-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: mai_framework
    volumes:
      - postgres_data:/var/lib/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d mai_framework"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - mai-network

  qdrant:
    image: qdrant/qdrant:latest
    container_name: mai-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-mai-qdrant-secret-key}
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'exec 3<>/dev/tcp/localhost/6333 && exec 3>&-'"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - mai-network

  mai-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mai-api
    ports:
      - "8000:8000"
    environment:
      - REDIS__URL=redis://redis:6379/0
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
      # LLM Provider configuration
      - LLM__PROVIDER=${LLM__PROVIDER:-lmstudio}
      # LM Studio (default, running on host)
      - LM_STUDIO__BASE_URL=${LM_STUDIO__BASE_URL:-http://host.docker.internal:1234/v1}
      - LM_STUDIO__MODEL_NAME=${LM_STUDIO__MODEL_NAME:-google/gemma-3-12b}
      # Ollama (when using docker-compose.ollama.yml or on host)
      - OLLAMA__BASE_URL=${OLLAMA__BASE_URL:-http://host.docker.internal:11434/v1}
      - OLLAMA__MODEL_NAME=${OLLAMA__MODEL_NAME:-llama3.2}
      # llama.cpp (when using docker-compose.llamacpp.yml or on host)
      - LLAMACPP__BASE_URL=${LLAMACPP__BASE_URL:-http://host.docker.internal:8080/v1}
      - LLAMACPP__MODEL_NAME=${LLAMACPP__MODEL_NAME:-local-model}
      # Database and Vector Store
      - DATABASE__URL=postgresql+asyncpg://postgres:postgres@postgres:5432/mai_framework
      - QDRANT__URL=http://qdrant:6333
      - QDRANT__API_KEY=${QDRANT_API_KEY:-mai-qdrant-secret-key}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - mai-network

  mai-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: mai-frontend
    ports:
      - "3000:3000"
    depends_on:
      mai-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    networks:
      - mai-network

  mai-gui:
    profiles:
      - gradio
    build:
      context: .
      dockerfile: Dockerfile.gui
    container_name: mai-gui
    ports:
      - "7860:7860"
    environment:
      - GUI_API_BASE_URL=http://mai-api:8000/api/v1
    depends_on:
      mai-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    networks:
      - mai-network

networks:
  mai-network:
    driver: bridge

volumes:
  redis_data:
    driver: local
  postgres_data:
    driver: local
  qdrant_data:
    driver: local
